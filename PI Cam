import cv2
import numpy as np
import tensorflow as tf
#from picamera.array import PiRGBArray
#from picamera import PiCamera
import time

# Load the TFLite model
interpreter = tf.lite.Interpreter(model_path="C:\\Users\\Shaan\\Downloads\\model5ep.tflite")  # Update the path for Raspberry Pi
interpreter.allocate_tensors()

# Get input and output tensors
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Emotion labels based on your provided folder structure
emotion_labels = ['anger', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']

# Preprocess the webcam image frame to the model's input requirements
def preprocess_image(frame):
    img = cv2.resize(frame, (260, 260))  # Resize frame to model input size (260x260)
    img = np.array(img, dtype=np.float32)
    img = img / 255.0  # Normalize if required
    img = np.expand_dims(img, axis=0)  # Add batch dimension
    return img

# Initialize the Raspberry Pi camera
camera = PiCamera()
camera.resolution = (640, 480)  # Original resolution for capturing
camera.framerate = 30
raw_capture = PiRGBArray(camera, size=(640, 480))

# Allow the camera to warm up
time.sleep(2)

# Capture frames from the camera
for frame in camera.capture_continuous(raw_capture, format="bgr", use_video_port=True):
    # Get the raw NumPy array representing the image
    image = frame.array
    
    # Preprocess the captured frame
    input_image = preprocess_image(image)
    
    # Set input tensor
    interpreter.set_tensor(input_details[0]['index'], input_image)
    
    # Run inference
    interpreter.invoke()
    
    # Get output tensor
    output_data = interpreter.get_tensor(output_details[0]['index'])
    
    # Get the predicted emotion
    predicted_class = np.argmax(output_data)
    predicted_emotion = emotion_labels[predicted_class]
    
    # Display the predicted emotion on the original frame
    cv2.putText(image, f"Emotion: {predicted_emotion}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)
    
    # Show the frame
    cv2.imshow('Emotion Detection', image)
    
    # Clear the stream to prepare for the next frame
    raw_capture.truncate(0)
    
    # Press 'q' to quit
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release resources
cv2.destroyAllWindows()
